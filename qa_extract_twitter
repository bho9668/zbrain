#!/usr/bin/env python
# -*- coding: utf-8 -*-

#
# Copyright 2019 Guenter Bartsch
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
#

import os
import sys
import codecs
import time
import json
import logging
import requests

from newspaper import Article

from optparse import OptionParser
from nltools  import misc

import twint

PROC_TITLE        = 'qa_extract_twitter'

TWITTER_CORPUSDIR = '/home/bofh/projects/ai/data/corpora/en/twitter'

QASRC_DIRFN       = 'data/qa_src'

# DEBUG_LIMIT       = 10
DEBUG_LIMIT       = 0

BLOCKLIST         = ['wapo.st', 'washingtonpost.com', 'twitter.com']

#
# init
#

misc.init_app(PROC_TITLE)

#
# commandline
#

parser = OptionParser("usage: %prog [options] corpus")

parser.add_option ("-v", "--verbose", action="store_true", dest="verbose",
                   help="verbose output")

(options, args) = parser.parse_args()

if options.verbose:
    logging.getLogger().setLevel(logging.DEBUG)
else:
    logging.getLogger().setLevel(logging.INFO)

# readability.readability

# logging.getLogger("readability").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

if len(args) != 1:
    parser.print_usage()
    sys.exit(1)

corpus_name = args[0]

#
# cleanup / preparation
#
 
cmd = 'mkdir -p %s/%s' % (QASRC_DIRFN, corpus_name)
logging.info(cmd)
os.system(cmd)

#
# find files
#

ls_files = []

for ls_a in os.listdir('%s/%s' % (TWITTER_CORPUSDIR, corpus_name)):

    ls_files.append('%s/%s/%s' % (TWITTER_CORPUSDIR, corpus_name, ls_a))

    if len(ls_files) % 100 == 0:
        logging.info ('%7d files...' % len(ls_files))

    if DEBUG_LIMIT and (len(ls_files)>=DEBUG_LIMIT):
        logging.warning('DEBUG_LIMIT reached at %d files.' % len(ls_files))
        break

logging.info('found %d files.' % len(ls_files))

#
# convert tweets
#

fncnt=0
for twitter_dumpfn in ls_files:

    fncnt += 1

    try:
        with open(twitter_dumpfn, 'r') as dumpf:
            data = json.loads(dumpf.read())
        if len(data['comments'])==0:
            continue

        jsonfn = '%s/%s/%s.json' % (QASRC_DIRFN, corpus_name, data['id'])
        if os.path.exists(jsonfn):
            continue

        url = data['textUrl'] if 'textUrl' in data else ''

        text = ''

        if url:

            skip = False
            for blocked_url in BLOCKLIST:
                if blocked_url in url:
                    skip = True
            if skip:
                continue

            logging.info ('%7d/%7d %s %s ... ' % (fncnt, len(ls_files), data['user'], url))

            article = Article(url)
            article.download()
            article.parse()

            text = article.text

            # print (text)

        # text

        if text:
            ds = {'info': text, 'date': data['date'], 'dlg': [data['text']]}
        else:
            ds = {'info': data['text'], 'date': data['date'], 'dlg': []}

        fav = 0
        for c in data['comments']:
            if c['favorites'] == 0:
                continue
            ds['dlg'].append(c['text'])
            fav += 1

        if (not text) and (fav == 0):
            continue

        # print(repr(ds))

        with open(jsonfn, 'w') as jsonf:
            jsonf.write(json.dumps(ds))

        logging.info ('%7d/%7d %s written. %s %s' % (fncnt, len(ls_files), jsonfn, data['user'], url[:30]))

    except:
        logging.exception('exception caught %s' % repr(data))

